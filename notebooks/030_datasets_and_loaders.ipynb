{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets and Dataloaders\n",
    "\n",
    "PyTorch helps us a lot with the data during training. `Dataloaders` create random splits of the data in every epoch of training for us, but they do need to know how to get the data and what the data is exactly. This is where the `Dataset` class comes in."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "Let's find a super simple dataset to work with. Many of you may be familiar with the iris dataset from R. We also have this in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename', 'data_module'])\n"
     ]
    }
   ],
   "source": [
    "# import some super basic data\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# load the iris dataset\n",
    "iris = load_iris(as_frame=True)\n",
    "print(iris.keys())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the `Dataset` class, we can prepare the iris dataset for PyTorch. We will need to inherit the PyTorch `Dataset` class and specify our `__init__()`, `__len__()` and `__getitem__()` constructors.\n",
    "\n",
    "The dataloader only accepts certain types of data. Among these are tensors and numpy arrays. So we should also know what format our data is in and change it if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "print(type(iris.data))\n",
    "print(type(iris.target))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The iris data is so far in dataframe format, so we will transform them into tensors in our `__init__()` constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Dataset class for the iris data\n",
    "\n",
    "# import torch and the Dataset class\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class IrisDataset(Dataset):\n",
    "    \"\"\"This is a child of Dataset providing the iris data to the dataloader\n",
    "    The init and getitem constructors are absolutely necessary\"\"\"\n",
    "    def __init__(self, data, targets):\n",
    "        super().__init__()\n",
    "        self.data = torch.Tensor(data.values)\n",
    "        self.targets = torch.Tensor(targets.values)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # for a given index, return the data and target\n",
    "        return self.data[idx,:], self.targets[idx]\n",
    "\n",
    "iris_dataset = IrisDataset(iris.data, iris.target)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was already the most complicated part in dealing with data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader\n",
    "\n",
    "Now we are ready to use a dataloader to provide us with random batches of our data during training. There is a lot in the docs that makes it look complicated, but don't worry. Getting started with dataloaders is really simple. For a while, you will not need more than this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.7000, 3.2000, 1.6000, 0.2000],\n",
      "        [5.9000, 3.0000, 5.1000, 1.8000],\n",
      "        [4.6000, 3.2000, 1.4000, 0.2000],\n",
      "        [6.9000, 3.1000, 5.1000, 2.3000],\n",
      "        [6.1000, 2.6000, 5.6000, 1.4000],\n",
      "        [5.5000, 2.4000, 3.8000, 1.1000],\n",
      "        [5.6000, 3.0000, 4.5000, 1.5000],\n",
      "        [5.2000, 2.7000, 3.9000, 1.4000]])\n",
      "tensor([0., 2., 0., 2., 2., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "# now we can create a dataloader\n",
    "from torch.utils.data import DataLoader\n",
    "iris_dataloader = DataLoader(iris_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# now we can iterate over the dataloader\n",
    "for data, target in iris_dataloader:\n",
    "    print(data)\n",
    "    print(target)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
