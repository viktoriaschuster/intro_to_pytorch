{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks in PyTorch\n",
    "\n",
    "Let's put our knowledge to use and see how one builds a neural network in PyTorch for a given task. If we stick to the iris dataset from before, we know that we have 4 features for every datapoint (flower) and want to predict the species.\n",
    "\n",
    "This already gives us a lot of information about what we should do. We want to build a classifier that takes the 4 features as input and spits out the probabilities of each species."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start with a class inheriting the `nn.Module` again as we have done before. Now we will use some more modules and functions that PyTorch provides in order to do actual machine learning. There are lots of nice building blocks that we can use, [have a look](https://pytorch.org/docs/stable/nn.html).\n",
    "\n",
    "We often call these building blocks `Layers`. They are also PyTorch `Modules` with specific parameters and operations that are executed for you. We will start very simple, with linear layers and rectified linear activation. The `Linear` layer applies a simple linear transformation to data $x$:\n",
    "\n",
    "$ y = xA^T + b$. with weight matrix $A$ and bias $b$ being trainable parameters of class `Parameter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's build a little classifier for iris\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# define the network\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, in_features:int, hidden_features:int, out_features:int):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear( # define the first fully connected layer\n",
    "            in_features,\n",
    "            hidden_features\n",
    "        )\n",
    "        self.fc2 = nn.Linear( # define the second fully connected layer\n",
    "            hidden_features,\n",
    "            out_features\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z = self.fc1(x) # apply the first fully connected layer\n",
    "        z = F.relu(z) # apply the relu activation function (non-linearity)\n",
    "        z = self.fc2(z) # apply the second fully connected layer\n",
    "        return F.softmax(z, dim=1) # apply the softmax activation function to return probabilities for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_classifier = Classifier(in_features=4, hidden_features=64, out_features=3) # create an instance of the network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have an example, we can get a better intuition about how the Linear layer works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "an input sample would have the shape:  torch.Size([1, 4])\n",
      "the first layer weights have the shape:  torch.Size([64, 4])\n",
      "the first layer bias has the shape:  torch.Size([64])\n",
      "the first layer output is of shape:  torch.Size([1, 64])\n"
     ]
    }
   ],
   "source": [
    "### printing some things that happen in the network forward pass and architecture\n",
    "dummy_input = torch.randn(1, 4)\n",
    "print(\"an input sample would have the shape: \", dummy_input.shape)\n",
    "print(\"the first layer weights have the shape: \", iris_classifier.fc1.weight.shape)\n",
    "print(\"the first layer bias has the shape: \", iris_classifier.fc1.bias.shape)\n",
    "print(\"the first layer output is of shape: \", iris_classifier.fc1(dummy_input).shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear layer does in principle this (but more efficient in C):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64])\n"
     ]
    }
   ],
   "source": [
    "pseudo_linear_output = torch.matmul(dummy_input, iris_classifier.fc1.weight.t()) + iris_classifier.fc1.bias\n",
    "print(pseudo_linear_output.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ReLU activation is one of the simplest and most popular activation functions and looks like this:\n",
    "\n",
    "<img src=\"images/ReLU_pytorch.png\" width=\"300\">\n",
    "\n",
    "(image from [PyTorch docs](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
